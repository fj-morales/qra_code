{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "# sys.path.append('qra_cod')\n",
    "from utils.meter import AUCMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "\n",
    "build_index_flag = 'yes'\n",
    "data_split = 'test'\n",
    "workdir = './baselines/workdir/'\n",
    "qloc = './qra_data/sprint/'\n",
    "galago_loc='./baselines/galago-3.10-bin/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sc(text):\n",
    "###    text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip())\n",
    "##    text = re.sub('[\\[\\]{}.,?;*!%^&_+():-]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip()) # DeepPaper method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "###     text = text.rstrip('.?')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_questions(filename):\n",
    "    with gzip.open(filename, 'rt') as tsv_in:\n",
    "        qreader = csv.reader(tsv_in, delimiter = '\\t')\n",
    "        questions = {}\n",
    "#         q_dict = {}\n",
    "        for q in qreader:\n",
    "            question = {}\n",
    "            if 'quora' in filename:\n",
    "                print('quora')\n",
    "#             elif 'sprint' in filename:\n",
    "#                 print('print')\n",
    "            else:\n",
    "#                 question['id'] = q[0]\n",
    "#                 q_dict[q[0]] = q[1] + ' ' + q[2]\n",
    "                question['title'] = q[1]\n",
    "                question['text'] = q[2]\n",
    "                questions[q[0]]=(dict(question))\n",
    "#         return [questions, q_dict]\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trectext_format(questions):\n",
    "    trec_questions = {}\n",
    "    for key, q in questions.items():\n",
    "        doc = '<DOC>\\n' + \\\n",
    "              '<DOCNO>' + key + '</DOCNO>\\n' + \\\n",
    "              '<TITLE>' + q['title'] + '</TITLE>\\n' + \\\n",
    "              '<TEXT>' + q['text'] + '</TEXT>\\n' + \\\n",
    "              '</DOC>\\n'\n",
    "        trec_questions[key] = doc\n",
    "    return trec_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trectext(trec_questions, filename):\n",
    "# Generate file to index\n",
    "#     with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "    with gzip.open(filename,'wt') as f_out:\n",
    "        for key, value in trec_questions.items():\n",
    "            f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(index_input, index_loc):\n",
    "    if build_index_flag == 'no':\n",
    "        return\n",
    "# Build corpus index \n",
    "    if not os.path.exists(index_loc):\n",
    "            os.makedirs(index_loc) \n",
    "    index_loc_param = '--indexPath=' + index_loc\n",
    "    galago_parameters = [galago_loc + 'galago', 'build', '--stemmer+krovetz']\n",
    "    galago_parameters.append('--inputPath+' + index_input)\n",
    "    galago_parameters.append(index_loc_param)\n",
    "    print(galago_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(galago_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dups(dups_file):\n",
    "    with open(dups_file, 'rt') as dups_in:\n",
    "        dup_reader = csv.reader(dups_in, delimiter = ' ')\n",
    "        dup_list = []\n",
    "        dup_dict = {}\n",
    "        for dup in dup_reader:\n",
    "            dup_dict['doc_id'] = dup[0]\n",
    "            dup_dict['dup_id'] = dup[1]\n",
    "            if 'pos' in dups_file:\n",
    "                dup_dict['label'] = 1\n",
    "            elif 'neg' in dups_file:\n",
    "                dup_dict['label'] = 0\n",
    "            dup_list.append(dict(dup_dict))\n",
    "    return dup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dup_files(dups_file):\n",
    "    with open(dups_file, 'rt') as dups_in:\n",
    "        dup_reader = csv.reader(dups_in, delimiter = ' ')\n",
    "        dup_list = []\n",
    "        for dup in dup_reader:\n",
    "#             print(dup)\n",
    "            if dup[0] in dup_dict.keys():\n",
    "                dup_dict[dup[0]].append(dup[1])\n",
    "            else:\n",
    "                dup_dict[dup[0]] = [dup[1]]\n",
    "    return dup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_file(questions, q_dup_pos, filename):\n",
    "    queries_list = []\n",
    "    queries_dict = {}\n",
    "    query = {}\n",
    "    for query in q_dup_pos:\n",
    "        key = query['doc_id']\n",
    "        q = questions[key]\n",
    "        text = remove_sc(q['title'] + ' ' + q['text']) #Join title and text \n",
    "        query['number'] = key\n",
    "#         query['text'] = '#stopword(' + text + ')'\n",
    "        query['text'] = '(' + text + ')'\n",
    "        queries_list.append(dict(query))\n",
    "    queries_dict['queries'] = queries_list\n",
    "    # with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "    with open(filename, 'wt') as q_file: #encoding option not working on python 2.7\n",
    "        json.dump(queries_dict, q_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 1 bm25 scored question = 'duplicated' question\n",
    "def get_bm25_docs(queries_file, q_all, index_loc, b_val=0.75, k_val=1.2):\n",
    "    index_loc_param = '--index=' + index_loc  \n",
    "    b=' --b=' + str(b_val)\n",
    "    k=' --k=' + str(k_val)\n",
    "    \n",
    "    command = galago_loc + 'galago threaded-batch-search --threadCount=50 --verbose=false \\\n",
    "         --casefold=true --requested=50000 ' + \\\n",
    "         index_loc_param + ' --scorer=bm25' + \\\n",
    "         b + \\\n",
    "         k + \\\n",
    "         '   ' + \\\n",
    "         queries_file + ' | cut -d\" \" -f1,3,5 '\n",
    "#          queries_file + ' | cut -d\" \" -f1,3,5 > all_results.txt'\n",
    "        # cut -d\" \" -f1,3' # for the document \n",
    "        \n",
    "    print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "#     galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, encoding='utf-8')\n",
    "    galago_bm25_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    (out, err) = galago_bm25_exec.communicate()\n",
    "    all_scores = out.splitlines()\n",
    "#     print(ids_docs)\n",
    "    \n",
    "#     return ids_docs\n",
    "    all_dict = {}\n",
    "    for doc in all_scores:\n",
    "        key_pair = doc.split(' ')[0] + '_' + doc.split(' ')[1]\n",
    "        all_dict[key_pair] = doc.split(' ')[2]\n",
    "    bm25_scores = [] \n",
    "    i = 0\n",
    "    for query_dict in q_all:\n",
    "        i += 1\n",
    "        key_pair = query_dict['doc_id'] + '_' + query_dict['dup_id']\n",
    "        try: \n",
    "            query_dict['score'] = all_dict[key_pair]\n",
    "        except:\n",
    "            query_dict['score'] = 0\n",
    "        if i % 10000 == 0:\n",
    "            print('processed: ', i)\n",
    "        bm25_scores.append(dict(query_dict))\n",
    "    return bm25_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_work_dirs():\n",
    "    if debug == 'yes':\n",
    "        print('yes')\n",
    "        # Execute remove sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = qloc.split('/')[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_prefix = workdir + dataset_name[0]\n",
    "index_loc = loc_prefix + '_index'\n",
    "questions_file = loc_prefix + '_questions' + '.gz'\n",
    "queries_file = loc_prefix + '_queries'\n",
    "trectext_file = loc_prefix + '_trectext.gz'\n",
    "index_input = trectext_file\n",
    "dups_file_pos = qloc + data_split + '.pos.txt'\n",
    "dups_file_neg = qloc + data_split + '.neg.txt'\n",
    "corpus_file = qloc + 'corpus.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = read_questions(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "q_dup_pos = read_dups(dups_file_pos)\n",
    "print(len(q_dup_pos))\n",
    "q_dup_neg = read_dups(dups_file_neg)\n",
    "print(len(q_dup_neg))\n",
    "q_all = q_dup_pos + q_dup_neg \n",
    "trec_questions = trectext_format(questions)\n",
    "save_trectext(trec_questions, trectext_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '7549_3', 'dup_id': '7549_0', 'label': 1},\n",
       " {'doc_id': '7550_1', 'dup_id': '7550_0', 'label': 1}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_dup_pos[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_file(questions, q_dup_pos, queries_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./baselines/galago-3.10-bin/bin/galago', 'build', '--stemmer+krovetz', '--inputPath+./baselines/workdir/sprint_trectext.gz', '--indexPath=./baselines/workdir/sprint_index']\n",
      "Running without server!\n",
      "Use --server=true to enable web-based status page.\n",
      "/ssd/home/francisco/msc_project/not-a-punching-bag/reproduction/qra_code/./baselines/workdir/sprint_trectext.gz detected as trectext\n",
      "Done Indexing.\n",
      "  - 0.01 Hours\n",
      "  - 0.32 Minutes\n",
      "  - 18.93 Seconds\n",
      "Documents Indexed: 31768.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "build_index(index_input, index_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baselines/galago-3.10-bin/bin/galago threaded-batch-search --threadCount=50 --verbose=false          --casefold=true --requested=1000 --index=./baselines/workdir/sprint_index --scorer=bm25 --b=0.75 --k=1.2   ./baselines/workdir/sprint_queries | cut -d\" \" -f1,3,5 \n",
      "('processed: ', 10000)\n",
      "('processed: ', 20000)\n",
      "('processed: ', 30000)\n",
      "('processed: ', 40000)\n",
      "('processed: ', 50000)\n",
      "('processed: ', 60000)\n",
      "('processed: ', 70000)\n",
      "('processed: ', 80000)\n",
      "('processed: ', 90000)\n",
      "('processed: ', 100000)\n"
     ]
    }
   ],
   "source": [
    "bm25_docs = get_bm25_docs(queries_file, q_all, index_loc, b_val=0.75, k_val=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '7549_3',\n",
       "  'dup_id': '7549_0',\n",
       "  'label': 1,\n",
       "  'number': '7549_3',\n",
       "  'score': '4.45191438',\n",
       "  'text': '(Franklin U722 USB modem signal strength Franklin U722 USB modem signal strength)'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_docs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [doc['score'] for doc in bm25_docs]\n",
    "scores = np.asarray(scores)\n",
    "scores = scores.astype(np.float)\n",
    "labels = [doc['label'] for doc in bm25_docs]\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AUC(0.05) = ', 0.9483761999999999)\n"
     ]
    }
   ],
   "source": [
    "auc_meter = AUCMeter()\n",
    "auc_meter.add(scores, labels)\n",
    "auc05_score = auc_meter.value(0.05)\n",
    "print('AUC(0.05) = ', auc05_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
