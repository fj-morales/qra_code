{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA: BM25+RM3 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re \n",
    "import csv\n",
    "import torch\n",
    "import sys\n",
    "# sys.path.append('qra_cod')\n",
    "from utils.meter import AUCMeter\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sc(text):\n",
    "###    text = re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip())\n",
    "##    text = re.sub('[\\[\\]{}.,?;*!%^&_+():-]', '', text.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\", '').strip()) # DeepPaper method\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text) # My method\n",
    "###     text = text.rstrip('.?')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_questions(filename):\n",
    "    with gzip.open(filename, 'rt') as tsv_in:\n",
    "        qreader = csv.reader(tsv_in, delimiter = '\\t')\n",
    "        questions = {}\n",
    "#         q_dict = {}\n",
    "        for q in qreader:\n",
    "            question = {}\n",
    "            if 'quora' in filename:\n",
    "                print('quora')\n",
    "#             elif 'sprint' in filename:\n",
    "#                 print('print')\n",
    "            else:\n",
    "#                 question['id'] = q[0]\n",
    "#                 q_dict[q[0]] = q[1] + ' ' + q[2]\n",
    "                question['title'] = q[1]\n",
    "                question['text'] = q[2]\n",
    "                questions[q[0]]=(dict(question))\n",
    "#         return [questions, q_dict]\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trectext_format(questions):\n",
    "    trec_questions = {}\n",
    "    for key, q in questions.items():\n",
    "        doc = '<DOC>\\n' + \\\n",
    "              '<DOCNO>' + key + '</DOCNO>\\n' + \\\n",
    "              '<TITLE>' + q['title'] + '</TITLE>\\n' + \\\n",
    "              '<TEXT>' + q['text'] + '</TEXT>\\n' + \\\n",
    "              '</DOC>\\n'\n",
    "        trec_questions[key] = doc\n",
    "    return trec_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trecfile(docs, filename, compression = 'yes'):\n",
    "    # Pickle to Trectext converter\n",
    "    doc_list = []\n",
    "    if compression == 'yes':\n",
    "        with gzip.open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt') as f_out:\n",
    "            docus = {}\n",
    "            for key, value in docs.items():\n",
    "                f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trectext(trec_questions, filename, compression = True):\n",
    "# Generate file to index\n",
    "#     with gzip.open(filename,'wt', encoding='utf-8') as f_out:\n",
    "    if compression == True:\n",
    "        with gzip.open(filename,'wt') as f_out:\n",
    "            for key, value in trec_questions.items():\n",
    "                f_out.write(value)\n",
    "    else:\n",
    "        with open(filename,'wt') as f_out:\n",
    "            for key, value in trec_questions.items():\n",
    "                f_out.write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(index_input, index_loc):\n",
    "    if build_index_flag == 'no':\n",
    "        return\n",
    "# Build corpus index \n",
    "    if os.path.exists(index_loc):\n",
    "        shutil.rmtree(index_loc)\n",
    "        os.makedirs(index_loc)\n",
    "    else:\n",
    "        os.makedirs(index_loc) \n",
    "#     index_loc_param = '--indexPath=' + index_loc\n",
    "\n",
    "    anserini_index = anserini_loc + 'target/appassembler/bin/IndexCollection'\n",
    "    anserini_parameters = [\n",
    "#                            'nohup', \n",
    "                           'sh',\n",
    "                           anserini_index,\n",
    "                           '-collection',\n",
    "                           'TrecCollection',\n",
    "                           '-generator',\n",
    "                           'JsoupGenerator',\n",
    "                           '-threads',\n",
    "                            '16',\n",
    "                            '-input',\n",
    "                           index_input,\n",
    "                           '-index',\n",
    "                           index_loc,\n",
    "                           '-storePositions',\n",
    "                            '-keepStopwords',\n",
    "                            '-storeDocvectors',\n",
    "                            '-storeRawDocs']\n",
    "#                           ' >& ',\n",
    "#                           log_file,\n",
    "#                            '&']\n",
    "\n",
    "\n",
    "\n",
    "#     anserini_parameters = ['ls',\n",
    "#                           index_loc]\n",
    "\n",
    "\n",
    "    print(anserini_parameters)\n",
    "\n",
    "    index_proc = subprocess.Popen(anserini_parameters,\n",
    "            stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = index_proc.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dups(dups_file):\n",
    "    with open(dups_file, 'rt') as dups_in:\n",
    "        dup_reader = csv.reader(dups_in, delimiter = ' ')\n",
    "        dup_list = []\n",
    "        dup_dict = {}\n",
    "        for dup in dup_reader:\n",
    "            dup_dict['doc_id'] = dup[0]\n",
    "            dup_dict['dup_id'] = dup[1]\n",
    "            if 'pos' in dups_file:\n",
    "                dup_dict['label'] = 1\n",
    "            elif 'neg' in dups_file:\n",
    "                dup_dict['label'] = 0\n",
    "            dup_list.append(dict(dup_dict))\n",
    "    return dup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dup_files(dups_file):\n",
    "    with open(dups_file, 'rt') as dups_in:\n",
    "        dup_reader = csv.reader(dups_in, delimiter = ' ')\n",
    "        dup_list = []\n",
    "        for dup in dup_reader:\n",
    "#             print(dup)\n",
    "            if dup[0] in dup_dict.keys():\n",
    "                dup_dict[dup[0]].append(dup[1])\n",
    "            else:\n",
    "                dup_dict[dup[0]] = [dup[1]]\n",
    "    return dup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries_file(questions, q_dup_pos, filename):\n",
    "    queries_list = []\n",
    "    queries_dict = {}\n",
    "    query = {}\n",
    "    id_num = 0\n",
    "    ids_dict = {}\n",
    "    q_trec = {}\n",
    "    for query in q_dup_pos:\n",
    "        str_id = str(id_num)\n",
    "        id_new = str_id.rjust(15, '0')\n",
    "        \n",
    "        key = query['doc_id']\n",
    "        q = questions[key]\n",
    "#         print(key)\n",
    "        text = remove_sc(q['title'] + ' ' + q['text']) #Join title and text \n",
    "        query['number'] = key\n",
    "#         query['text'] = '#stopword(' + text + ')'\n",
    "        query['text'] = '(' + text + ')'\n",
    "        queries_list.append(dict(query))\n",
    "        \n",
    "        q_t = '<top>\\n\\n' + \\\n",
    "          '<num> Number: ' + id_new + '\\n' + \\\n",
    "          '<title> ' + text + '\\n\\n' + \\\n",
    "          '<desc> Description:' + '\\n\\n' + \\\n",
    "          '<narr> Narrative:' + '\\n\\n' + \\\n",
    "          '</top>\\n\\n'\n",
    "        q_trec[key] = q_t\n",
    "#         print(q)\n",
    "        ids_dict[str(id_num)] = key\n",
    "        id_num += 1\n",
    "        \n",
    "    queries_dict['queries'] = queries_list\n",
    "    # with open(filename, 'wt', encoding='utf-8') as q_file:\n",
    "    with open(filename, 'wt') as q_file: #encoding option not working on python 2.7\n",
    "        json.dump(queries_dict, q_file, indent = 4)\n",
    "        \n",
    "    return [q_trec, ids_dict]\n",
    "        \n",
    "        ########################\n",
    "        ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b=0.2, k=0.8, N=10, M=10, Lambda=0.5):\n",
    "    print(hits)\n",
    "    anserini_search = anserini_loc + 'target/appassembler/bin/SearchCollection'\n",
    "#     print(b_val)\n",
    "    command = [ \n",
    "               'sh',\n",
    "               anserini_search,\n",
    "               '-topicreader',\n",
    "                'Trec',\n",
    "                '-index',\n",
    "                index_loc,\n",
    "                '-topics',\n",
    "                q_topics_file,\n",
    "                '-output',\n",
    "                retrieved_docs_file,\n",
    "                '-bm25',\n",
    "                '-b',\n",
    "                str(b),\n",
    "                '-k1',\n",
    "                str(k),\n",
    "#                 '-rm3',\n",
    "#                 '-rm3.fbDocs',\n",
    "#                 str(N),\n",
    "#                 '-rm3.fbTerms',\n",
    "#                 str(M),\n",
    "#                 '-rm3.originalQueryWeight',\n",
    "#                 str(Lambda),\n",
    "                '-hits',\n",
    "                str(hits), \n",
    "                '-threads',\n",
    "                '10'\n",
    "               ]\n",
    "    print(command)\n",
    "#     command = command.encode('utf-8')\n",
    "    anserini_exec = subprocess.Popen(command, stdout=subprocess.PIPE, shell=False)\n",
    "    (out, err) = anserini_exec.communicate()\n",
    "    print(out)\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 1 bm25 scored question = 'duplicated' question\n",
    "# Return top 100 bm25 scored docs, given query and corpus indexed by anserini\n",
    "\n",
    "def generate_preds_file(retrieved_docs_file, q_all, ids_dict, hits):\n",
    "    \n",
    "    with open(retrieved_docs_file, 'rt') as f_in:\n",
    "        all_dict = {}\n",
    "        for doc in f_in:\n",
    "#             print(doc)\n",
    "            id_aux = doc.split(' ')[0]\n",
    "            current_key = ids_dict[id_aux]\n",
    "            key_pair = current_key + '_' + doc.split(' ')[2]\n",
    "            all_dict[key_pair] = doc.split(' ')[4]\n",
    "        bm25_scores = [] \n",
    "        i = 0\n",
    "        for query_dict in q_all:\n",
    "            i += 1\n",
    "            key_pair = query_dict['doc_id'] + '_' + query_dict['dup_id']\n",
    "            try: \n",
    "                query_dict['score'] = all_dict[key_pair]\n",
    "            except:\n",
    "                query_dict['score'] = 0\n",
    "            if i % 10000 == 0:\n",
    "                print('processed: ', i)\n",
    "            bm25_scores.append(dict(query_dict))\n",
    "        return bm25_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         aux_var = -1\n",
    "#         bm25_docs = []\n",
    "#         while aux_var != 0:\n",
    "#             question = {}\n",
    "#             lines_gen = islice(f_in, hits)\n",
    "#             documents = []\n",
    "#             for line in lines_gen:\n",
    "#                 id_aux = line.split(' ')[0]\n",
    "#                 current_key = ids_dict[id_aux]\n",
    "#                 documents.append(line.split(' ')[2])\n",
    "                \n",
    "# ###             print(documents)\n",
    "#             aux_var = len(documents)\n",
    "#             if aux_var == 0: \n",
    "#                 break\n",
    "# # ##            print(aux_var)##\n",
    "# # ##            print(documents)\n",
    "#             question['id'] = current_key\n",
    "#             question['body'] = q_dict[current_key]\n",
    "            \n",
    "#             if \"bioasq\" in dataset_name: \n",
    "#                 documents_url = ['http://www.ncbi.nlm.nih.gov/pubmed/' + doc for doc in documents]\n",
    "#                 question['documents'] = documents_url\n",
    "#             elif \"rob04\" in dataset_name:\n",
    "#                 question['documents'] = documents\n",
    "#             bm25_docs.append(dict(question))\n",
    "            \n",
    "#     return bm25_docs        \n",
    "\n",
    "\n",
    "\n",
    "# def get_bm25_docs(queries_file, q_all, index_loc, b_val=0.75, k_val=1.2):\n",
    "    \n",
    "#     all_scores = out.splitlines()\n",
    "# #     print(ids_docs)\n",
    "    \n",
    "# #     return ids_docs\n",
    "#     all_dict = {}\n",
    "#     for doc in all_scores:\n",
    "#         key_pair = doc.split(' ')[0] + '_' + doc.split(' ')[2]\n",
    "#         all_dict[key_pair] = doc.split(' ')[4]\n",
    "#     bm25_scores = [] \n",
    "#     i = 0\n",
    "#     for query_dict in q_all:\n",
    "#         i += 1\n",
    "#         key_pair = query_dict['doc_id'] + '_' + query_dict['dup_id']\n",
    "#         try: \n",
    "#             query_dict['score'] = all_dict[key_pair]\n",
    "#         except:\n",
    "#             query_dict['score'] = 0\n",
    "#         if i % 10000 == 0:\n",
    "#             print('processed: ', i)\n",
    "#         bm25_scores.append(dict(query_dict))\n",
    "#     return bm25_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_work_dirs():\n",
    "    if debug == 'yes':\n",
    "        print('yes')\n",
    "        # Execute remove sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(file, preds):\n",
    "    with open(file, 'wt') as f_out:\n",
    "        json.dump(preds, f_out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_process():\n",
    "    print( 'Starting', multiprocessing.current_process().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AUC(0.05) = ', 0.7911586061733018)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(baseline_preds):\n",
    "\n",
    "#     baseline_preds[0:1]\n",
    "    \n",
    "    scores = [doc['score'] for doc in baseline_preds]\n",
    "    scores = np.asarray(scores)\n",
    "    scores = scores.astype(np.float)\n",
    "    labels = [doc['label'] for doc in baseline_preds]\n",
    "    labels = np.asarray(labels)\n",
    "    labels = labels.astype(np.int)\n",
    "    \n",
    "    auc_meter = AUCMeter()\n",
    "    auc_meter.add(scores, labels)\n",
    "    auc05_score = auc_meter.value(0.05)\n",
    "    print('AUC(0.05) = ', auc05_score)\n",
    "    return auc05_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_computing(params):\n",
    "    b = params[0]\n",
    "    k = params[1]\n",
    "    N = params[2]\n",
    "    M = params[3]\n",
    "    Lambda = params[4]\n",
    "\n",
    "    params_suffix = 'b' + str(b) + 'k' + str(k) + 'N' + str(N) + 'M' + str(M) + 'Lambda' + str(Lambda)\n",
    "    retrieved_docs_file = workdir + 'run_bm25_rm3_preds_' + dataset_name[0] + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "\n",
    "    retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b, k, N, M, Lambda)\n",
    "    baseline_preds = generate_preds_file(retrieved_docs_file, q_all, ids_dict, hits)\n",
    "#     save_preds(baseline_preds_file, baseline_preds)  \n",
    "    auc05_score = evaluate(baseline_preds)\n",
    "    \n",
    "    results = [\n",
    "        b,\n",
    "        k,\n",
    "        N,\n",
    "        M,\n",
    "        Lambda,\n",
    "        float(auc_05_score)\n",
    "    ]\n",
    "    os.remove(retrieved_docs_file)\n",
    "#     os.remove(baseline_preds_file)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dev_model(best_model_params_file, random_iterations):\n",
    "#     random_search = 'yes'\n",
    "    \n",
    "    if random_search == 'yes':\n",
    "        ## Heavy random search\n",
    "        brange = np.arange(0.1,1,0.05)\n",
    "        krange = np.arange(0.1,4,0.1)\n",
    "        N_range = np.arange(5,500,1) # num of docs\n",
    "        M_range = np.arange(5,500,1) # num of terms\n",
    "        lamb_range = np.arange(0,1,0.1) # weights of original query\n",
    "\n",
    "        ## Light random search\n",
    "#         brange = [0.2]\n",
    "#         krange = [0.8]\n",
    "#         N_range = np.arange(1,50,2)\n",
    "#         M_range = np.arange(1,50,2)\n",
    "#         lamb_range = np.arange(0,1,0.2)\n",
    "        \n",
    "        h_param_ranges = [brange, krange, N_range, M_range, lamb_range]\n",
    "        params = get_random_params(h_param_ranges, random_iterations)\n",
    "\n",
    "    else:\n",
    "        brange = [0.2]\n",
    "        krange = [0.8]\n",
    "        N_range = [11]\n",
    "        M_range = [10]\n",
    "        lamb_range = [0.5]\n",
    "       \n",
    "        params = [[round(b,3), round(k,3), round(N,3), round(M,3), round(Lambda,3)] \n",
    "                  for b in brange for k in krange for N in N_range for M in M_range for Lambda in lamb_range]\n",
    "    \n",
    "    pool_size = 20\n",
    "#     print(len(params))\n",
    "    pool = multiprocessing.Pool(processes=pool_size,\n",
    "                                initializer=start_process,\n",
    "                                )\n",
    "\n",
    "#     pool_outputs = pool.map(baseline_computing, params)\n",
    "    \n",
    "\n",
    "    pool_outputs = pool.map_async(baseline_computing, params)\n",
    "#     pool_outputs.get()\n",
    "    ###\n",
    "\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    \n",
    "    pool.close() # no more tasks\n",
    "    while (True):\n",
    "        if (pool_outputs.ready()): break\n",
    "        remaining = pool_outputs._number_left\n",
    "#         remaining2 = remaining1\n",
    "#         remaining1 = pool_outputs._number_left\n",
    "        if remaining%10 == 0:\n",
    "            print(\"Waiting for\", remaining, \"tasks to complete...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "      \n",
    "    pool.join()  # wrap up current tasks\n",
    "    pool_outputs.get()\n",
    "    params_file = './baselines/best_ir_model/' + dataset_name[0] + '_' + 'bm25_rm3_' + data_split + '_hparams.pickle'\n",
    "    pickle.dump(pool_outputs.get(), open(params_file, \"wb\" ) )\n",
    "    print('Total parameters: ' + str(len(pool_outputs.get())))\n",
    "    best_model_params = max(pool_outputs.get(), key=lambda x: x[5])\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'b': best_model_params[0],\n",
    "        'k': best_model_params[1],\n",
    "        'N': best_model_params[2],\n",
    "        'M': best_model_params[3],\n",
    "        'Lambda': best_model_params[4],\n",
    "        'random_iterations': random_iterations,\n",
    "        'auc_05_score': best_model_params[5],\n",
    "    }\n",
    "    best_model_dict = {k:str(v) for k, v in best_model_dict.items()} # everything to string\n",
    "    \n",
    "    with open(best_model_params_file, 'wt') as best_model_f:\n",
    "        json.dump(best_model_dict, best_model_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n",
      "113300\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        qloc = sys.argv[1] + '/'\n",
    "        print(qloc)\n",
    "        data_split = sys.argv[2]\n",
    "        \n",
    "    except:\n",
    "        sys.exit(\"Provide data location, split, and number of random iterations\")\n",
    "    \n",
    "    try:\n",
    "        num_random_iter = sys.argv[3]\n",
    "    except: \n",
    "        if 'dev' in data_split:\n",
    "            print('No number random of random iterations provided. Using default = 5000')\n",
    "            num_random_iter = 5000\n",
    "        elif 'test' in data_split:\n",
    "            print('No need for random iterations in test mode.')\n",
    "            num_random_iter = 1\n",
    "    \n",
    "    ## Options\n",
    "\n",
    "    build_index_flag = 'yes'\n",
    "    \n",
    "    \n",
    "#     data_split = 'test'\n",
    "    \n",
    "    \n",
    "    workdir = './baselines/workdir/'\n",
    "    hits = 100\n",
    "    \n",
    "    \n",
    "    \n",
    "#     qloc = './qra_data/apple/' # Must be a commandline parameter\n",
    "    \n",
    "    \n",
    "    \n",
    "    galago_loc='./baselines/galago-3.10-bin/bin/'\n",
    "    trec_storage = '/ssd/francisco/trec_datasets/qra/'\n",
    "    \n",
    "    to_index_files ='./baselines/to_index_files/'\n",
    "    anserini_loc = '../anserini/'\n",
    "\n",
    "    dataset_name = qloc.split('/')[-2:]\n",
    "    \n",
    "    if not os.path.exists(workdir): \n",
    "        os.makedirs(workdir)\n",
    "    \n",
    "    if os.path.exists(to_index_files): \n",
    "        shutil.rmtree(to_index_files)\n",
    "        os.makedirs(to_index_files)\n",
    "    else:\n",
    "        os.makedirs(to_index_files)\n",
    "    \n",
    "    loc_prefix = workdir + dataset_name[0]\n",
    "    index_loc = loc_prefix + '_anserini_index'\n",
    "    questions_file = loc_prefix + '_questions' + '.gz'\n",
    "    queries_file = loc_prefix + '_queries'\n",
    "    trectext_file = to_index_files + dataset_name[0] + '_trectext.gz' # needs to be alone, no other files in the same directory when indexing\n",
    "    trectext_doc_file = trec_storage + dataset_name[0] + '_trectext'\n",
    "    \n",
    "    index_input = to_index_files\n",
    "    dups_file_pos = qloc + data_split + '.pos.txt'\n",
    "    dups_file_neg = qloc + data_split + '.neg.txt'\n",
    "    corpus_file = qloc + 'corpus.tsv.gz' \n",
    "    \n",
    "    build_index(index_input, index_loc)\n",
    "    \n",
    "    questions = read_questions(corpus_file)\n",
    "    \n",
    "    q_dup_pos = read_dups(dups_file_pos)\n",
    "    print(len(q_dup_pos))\n",
    "    q_dup_neg = read_dups(dups_file_neg)\n",
    "    print(len(q_dup_neg))\n",
    "    q_all = q_dup_pos + q_dup_neg \n",
    "    trec_questions = trectext_format(questions)\n",
    "    save_trectext(trec_questions, trectext_file)\n",
    "    save_trectext(trec_questions, trectext_doc_file, compression = False)\n",
    "    \n",
    "    q_dup_pos[0:2]\n",
    "    \n",
    "    \n",
    "    print(queries_file)\n",
    "    q_topics_file = loc_prefix + '_query'\n",
    "    [q_trec, ids_dict] = generate_queries_file(questions, q_dup_pos, queries_file)\n",
    "    to_trecfile(q_trec, q_topics_file, compression = 'no')\n",
    "    ids_equivalence_filename = dataset_name[0] + '_' + 'ids_equivalence'  + '_' + data_split + '.txt'\n",
    "    ids_equivalence_file = trec_storage + ids_equivalence_filename\n",
    "    with open(ids_equivalence_file, 'wt') as outfile:\n",
    "        json.dump(ids_dict, outfile)\n",
    "    \n",
    "    best_model_params_file = workdir + dataset_name[0] + '_bm25_rm3_best_model_dev.json'\n",
    "    \n",
    "    if 'dev' in split:\n",
    "        print('Dev Mode')\n",
    "        find_best_dev_model(best_model_params_file, int(num_random_iter))\n",
    "    if 'test' in split:\n",
    "        print('Test Mode')\n",
    "        test_results = get_test_metrics(best_model_params_file)\n",
    "        print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sh', '../anserini/target/appassembler/bin/IndexCollection', '-collection', 'TrecCollection', '-generator', 'JsoupGenerator', '-threads', '16', '-input', './baselines/to_index_files/', '-index', './baselines/workdir/apple_anserini_index', '-storePositions', '-keepStopwords', '-storeDocvectors', '-storeRawDocs']\n",
      "2019-05-03 07:38:26,376 INFO  [main] index.IndexCollection (IndexCollection.java:429) - DocumentCollection path: ./baselines/to_index_files/\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:430) - Index path: ./baselines/workdir/apple_anserini_index\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:431) - CollectionClass: TrecCollection\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:432) - Generator: JsoupGenerator\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:433) - Threads: 16\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:434) - Stemmer: porter\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:435) - Keep stopwords? true\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:436) - Store positions? true\n",
      "2019-05-03 07:38:26,378 INFO  [main] index.IndexCollection (IndexCollection.java:437) - Store docvectors? true\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:438) - Store transformed docs? false\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:439) - Store raw docs? true\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:440) - Optimize (merge segments)? false\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:441) - Whitelist: null\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:442) - Solr? false\n",
      "2019-05-03 07:38:26,379 INFO  [main] index.IndexCollection (IndexCollection.java:453) - Dry run (no index created)? false\n",
      "2019-05-03 07:38:26,386 INFO  [main] index.IndexCollection (IndexCollection.java:536) - Starting indexer...\n",
      "2019-05-03 07:38:26,595 INFO  [main] index.IndexCollection (IndexCollection.java:563) - 1 files found in ./baselines/to_index_files\n",
      "2019-05-03 07:38:39,175 INFO  [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:295) - to_index_files/apple_trectext.gz: 80466 docs added.\n",
      "2019-05-03 07:38:40,726 INFO  [main] index.IndexCollection (IndexCollection.java:638) - # Final Counter Values\n",
      "2019-05-03 07:38:40,726 INFO  [main] index.IndexCollection (IndexCollection.java:639) - indexed:           80,466\n",
      "2019-05-03 07:38:40,726 INFO  [main] index.IndexCollection (IndexCollection.java:640) - empty:                  0\n",
      "2019-05-03 07:38:40,727 INFO  [main] index.IndexCollection (IndexCollection.java:641) - unindexed:              0\n",
      "2019-05-03 07:38:40,727 INFO  [main] index.IndexCollection (IndexCollection.java:642) - unindexable:            0\n",
      "2019-05-03 07:38:40,727 INFO  [main] index.IndexCollection (IndexCollection.java:643) - skipped:                0\n",
      "2019-05-03 07:38:40,727 INFO  [main] index.IndexCollection (IndexCollection.java:644) - errors:                 0\n",
      "2019-05-03 07:38:40,735 INFO  [main] index.IndexCollection (IndexCollection.java:647) - Total 80,466 documents indexed in 00:00:14\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baselines/workdir/apple_queries\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./baselines/workdir/run_bm25_rm3_preds_apple_test_b0.75k1.2N10M10Lambda0.5.txt\n",
      "1000\n",
      "['sh', '../anserini/target/appassembler/bin/SearchCollection', '-topicreader', 'Trec', '-index', './baselines/workdir/apple_anserini_index', '-topics', './baselines/workdir/apple_query', '-output', './baselines/workdir/run_bm25_rm3_preds_apple_test_b0.75k1.2N10M10Lambda0.5.txt', '-bm25', '-b', '0.75', '-k1', '1.2', '-hits', '1000', '-threads', '10']\n",
      "2019-05-03 07:38:41,990 INFO  [main] search.SearchCollection (SearchCollection.java:183) - Reading index at ./baselines/workdir/apple_anserini_index\n",
      "2019-05-03 07:38:42,252 INFO  [main] search.SearchCollection (SearchCollection.java:203) - Use Bag of Terms query\n",
      "2019-05-03 07:38:42,349 INFO  [pool-2-thread-1] search.SearchCollection$SearcherThread (SearchCollection.java:134) - [Start] Ranking with similarity: BM25(k1=1.2,b=0.75)\n",
      "2019-05-03 07:39:37,534 INFO  [pool-2-thread-1] search.SearchCollection$SearcherThread (SearchCollection.java:166) - [Finished] Ranking with similarity: BM25(k1=1.2,b=0.75)\n",
      "2019-05-03 07:39:37,541 INFO  [pool-2-thread-1] search.SearchCollection$SearcherThread (SearchCollection.java:167) - Run 1113 topics searched in 00:00:55\n",
      "2019-05-03 07:39:37,569 INFO  [main] search.SearchCollection (SearchCollection.java:499) - Total run time: 00:00:55\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# b=0.75\n",
    "# k=1.2\n",
    "# N=10\n",
    "# M=10\n",
    "# Lambda=0.5\n",
    "\n",
    "# params_suffix = 'b' + str(b) + 'k' + str(k) + 'N' + str(N) + 'M' + str(M) + 'Lambda' + str(Lambda)\n",
    "# retrieved_docs_file = workdir + 'run_bm25_rm3_preds_' + dataset_name[0] + '_' + data_split + '_' + params_suffix + '.txt'\n",
    "# print(retrieved_docs_file)\n",
    "# retrieve_docs(q_topics_file, retrieved_docs_file, index_loc, hits, b, k, N, M, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('processed: ', 10000)\n",
      "('processed: ', 20000)\n",
      "('processed: ', 30000)\n",
      "('processed: ', 40000)\n",
      "('processed: ', 50000)\n",
      "('processed: ', 60000)\n",
      "('processed: ', 70000)\n",
      "('processed: ', 80000)\n",
      "('processed: ', 90000)\n",
      "('processed: ', 100000)\n",
      "('processed: ', 110000)\n"
     ]
    }
   ],
   "source": [
    "    baseline_docs = generate_preds_file(retrieved_docs_file, q_all, ids_dict, hits)\n",
    "    save_preds(baseline_preds_file, baseline_preds) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
